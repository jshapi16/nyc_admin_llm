{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85763219-f770-41ba-b2d9-f7e8a83ce9ec",
   "metadata": {},
   "source": [
    "# This notebook can Retrieve and Generate information about Titles 8, 9 and 10 of the NYC Administrative Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df235c9f-0d1f-4366-b1af-d2273bffe004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8a729e-8a95-4ad5-8f90-03b1cfeb5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import json\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679e096e-cb41-44a9-a955-3038846dac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections: ['df_810_bart']\n"
     ]
    }
   ],
   "source": [
    "#Check the chromadb collection is accessible\n",
    "client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "\n",
    "# List all collections in your ChromaDB instance\n",
    "collections = client.list_collections()\n",
    "\n",
    "# Print the collections to see if \"df_810\" exists\n",
    "print(\"Available collections:\", collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cc29a2-a6b1-464e-8236-c76074bd0872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding dimension: 1024\n",
      "{'ids': [['4045', '2369', '2441', '2442', '4189']], 'embeddings': None, 'documents': [['11. Permit. The permit for purchase and possession of rifles and shotguns issued by the commissioner.Who issues permits for the purchase and possession of rifles and shotguns in NYC?According to NYC Administrative Code Title 10, Section 10.301(11), the commissioner issues permits for the purchase and possession of rifles and shotguns.', 'b. Air pistols and air rifles; selling or possessing.Are there age restrictions for purchasing air pistols and air rifles in NYC?The provided excerpt from NYC Administrative Code Section 10.131(b)', 'h. Rifles and shotguns; carrying or possessing.Under what conditions can a person legally carry or possess rifles and shotguns in NYC?According to NYC Administrative Code Title 10, Section 10.131(h), a person may legally carry or possess rifles and shotguns if they have obtained the proper permits and licenses as required by local law. Rifles and shotguns must be registered with the city, and the person must comply with all applicable storage, transportation, and usage regulations. Specific restrictions apply to certain locations such as schools, government buildings, and public gatherings.', \"h. Rifles and shotguns; carrying or possessing.Who is responsible for issuing rifle and shotgun permits in NYC?The New York City Police Department is responsible for issuing rifle and shotgun permits in NYC, as specified in the NYC Administrative Code Title 10, Section 10.131. The NYPD's License Division handles the application process, background checks, and issuance of permits for rifles and shotguns within city limits.\", 'o. Rules and regulations.Who is responsible for establishing rules for firearm dealers in NYC?The police commissioner is responsible for establishing rules and regulations for firearm dealers in NYC, as specified in Section 10.302(o) of the NYC Administrative Code.']], 'uris': None, 'data': None, 'metadatas': [[{'chapter/subchapter_num': 3.0, 'chapter_title': 'Firearms', 'section_number': '10.301', 'section_title': 'Control and regulation of the disposition, purchase and possession of firearms, rifles, shotguns and assault weapons', 'title_name': 'Public Safety. ', 'title_num': 10}, {'chapter/subchapter_num': 1.0, 'chapter_title': 'Public Safety', 'section_number': '10.131', 'section_title': 'Firearms', 'title_name': 'Public Safety. ', 'title_num': 10}, {'chapter/subchapter_num': 1.0, 'chapter_title': 'Public Safety', 'section_number': '10.131', 'section_title': 'Firearms', 'title_name': 'Public Safety. ', 'title_num': 10}, {'chapter/subchapter_num': 1.0, 'chapter_title': 'Public Safety', 'section_number': '10.131', 'section_title': 'Firearms', 'title_name': 'Public Safety. ', 'title_num': 10}, {'chapter/subchapter_num': 3.0, 'chapter_title': 'Firearms', 'section_number': '10.302', 'section_title': 'Licensing of gunsmiths, of wholesale manufacturers of firearms, or assemblers of firearms, dealers in firearms, dealers in rifles and shotguns, and special theatrical dealers', 'title_name': 'Public Safety. ', 'title_num': 10}]], 'distances': [[252.1258087158203, 255.4981231689453, 258.1777648925781, 258.7419738769531, 268.6162414550781]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "#Check the query is embedded and the documents are being retrieved successfully\n",
    "# Load BART model and tokenizer\n",
    "model_name = \"facebook/bart-large\"  # You can change this to a different variant if needed\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartModel.from_pretrained(model_name)\n",
    "\n",
    "# Specify the path where ChromaDB stores the collection\n",
    "chroma_path = \"./chromadb\"  # You can modify this to any path you prefer\n",
    "chroma_client = chromadb.PersistentClient(path=chroma_path)\n",
    "\n",
    "# Assuming your collection is called \"df_810_bart\" (you can change it to the name you prefer)\n",
    "collection = chroma_client.get_collection(\"df_810_bart\")\n",
    "\n",
    "def encode_text_with_bart(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    # Get the embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # We take the last hidden state of the model (output from BART)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling across tokens\n",
    "    # Ensure the embedding is exactly 1024 dimensions\n",
    "    if embeddings.shape[1] < 1024:\n",
    "        # If smaller, pad with zeros\n",
    "        padding = torch.zeros(1, 1024 - embeddings.shape[1])\n",
    "        embeddings = torch.cat([embeddings, padding], dim=1)\n",
    "    elif embeddings.shape[1] > 1024:\n",
    "        # If larger, truncate\n",
    "        embeddings = embeddings[:, :1024]\n",
    "    return embeddings.squeeze().tolist()  # Convert tensor to list\n",
    "\n",
    "# Now use this function for both storing and querying\n",
    "query_text = \"What are the restrictions on carrying guns in NYC?\"\n",
    "query_embedding = encode_text_with_bart(query_text)\n",
    "print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
    "\n",
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5,\n",
    "    include=['documents', 'metadatas', 'distances']\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d5e8a-dfc6-42e1-bedc-017a73086ab9",
   "metadata": {},
   "source": [
    "# RAG #1: Open Source RAG application embeded with Facebook/Bart and generated with Google/Flan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ef7fda-aa2c-45b4-a82f-38e8d5c99c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Rifles and shotguns must be registered with the city, and the person must comply with all applicable storage, transportation, and usage regulations.\n"
     ]
    }
   ],
   "source": [
    "#This code runs the model by embedding the query using Facebook/Bart and generating an answer using Google/Flan\n",
    "import chromadb\n",
    "from transformers import BartTokenizer, BartModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import torch.mps  # Ensures Metal is utilized on Mac\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "token = \"your_token_here\"  # Use your hugging face token for authentication\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    You are an AI assistant specializing in NYC legal regulations. \n",
    "    Your task is to provide legally accurate and precise responses for a non-legal expert, \n",
    "    based **only** on the provided legal text. \n",
    "\n",
    "    **Do not make assumptions** or provide information outside the given context.\n",
    "\n",
    "    ---\n",
    "    \n",
    "    ### Legal Text:\n",
    "    {context}\n",
    "\n",
    "    ### User Question:\n",
    "    {question}\n",
    "\n",
    "    ### Answer:\n",
    "    Provide a clear and concise response based strictly on the provided legal text.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Initialize ChromaDB client and collection\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb\")  # Ensure this points to your ChromaDB path\n",
    "collection = chroma_client.get_collection(\"df_810_bart\")  # Use your collection name\n",
    "\n",
    "# Load the embedding model (BART)\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartModel.from_pretrained(model_name)\n",
    "\n",
    "# Load the smaller model (Flan-T5-small)\n",
    "llm_name = \"google/flan-t5-base\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "\n",
    "# Ensure the model runs efficiently on macOS using MPS\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.float32,  # MPS prefers float32 over float16\n",
    "    device_map=\"auto\"  # Let the system decide the best device (MPS or CPU)\n",
    ")\n",
    "\n",
    "# Use Metal (MPS) if available\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)  # Move the embedding model to the device\n",
    "llm_model.to(device)  # Move the LLM model to the device\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Generate embeddings using BART.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "def retrieve_docs(query, top_k=3):\n",
    "    \"\"\"Retrieve relevant documents from ChromaDB.\"\"\"\n",
    "    query_embedding = embed_text(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "    return results[\"documents\"][0] if results[\"documents\"] else []\n",
    "\n",
    "def generate_answer(query):\n",
    "    \"\"\"Retrieve documents and use the LLM to generate an answer.\"\"\"\n",
    "    retrieved_docs = retrieve_docs(query)\n",
    "    context = \"\\n\".join(retrieved_docs) if retrieved_docs else \"No relevant documents found.\"\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = prompt_template.format(context=context, question=query)\n",
    "\n",
    "    # Tokenize input and send to the device (MPS/CPU)\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        output = llm_model.generate(**inputs, max_length=512)\n",
    "\n",
    "    return llm_tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e988e780-86a0-4cd1-8e20-2782498e13a7",
   "metadata": {},
   "source": [
    "# RAG #1: QUERY THE OPEN SOURCE (GOOGLE/FLAN) GENERATED ANSWER HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9723e-8463-4530-849a-c240949e1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the query text\n",
    "query = \"What are the restrictions on carrying guns in NYC?\"\n",
    "answer = generate_answer(query)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e916108-0206-4413-a36d-9bf33b48e46f",
   "metadata": {},
   "source": [
    "# RAG #2: This version uses Facebook/Bart for embedding and then Anthropic's Claude for generating an answer it requires an Anthropic api_key. Queries cost about 1¢."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a2603c-6265-41c0-94df-ba1f4a43e127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based solely on the provided legal text, I can only share limited information about restrictions on carrying guns in NYC, specifically about rifles and shotguns:\n",
      "\n",
      "According to NYC Administrative Code Title 10, Section 10.131(h), a person may legally carry or possess rifles and shotguns if they:\n",
      "1. Have obtained the proper permits and licenses as required by local law\n",
      "2. Have registered the rifles and shotguns with the city\n",
      "3. Comply with all applicable storage, transportation, and usage regulations\n",
      "\n",
      "The text also mentions that specific restrictions apply to certain locations such as schools, government buildings, and public gatherings, but does not provide details about these restrictions.\n",
      "\n",
      "The legal text does not provide information about restrictions on carrying other types of firearms beyond rifles and shotguns, nor does it specify the exact nature of the storage, transportation, and usage regulations mentioned.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "import torch.mps  # Ensures Metal is utilized on Mac\n",
    "from langchain.prompts import PromptTemplate\n",
    "import anthropic\n",
    "\n",
    "api_key = \"anthropic_api_key\"\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant specializing in NYC legal regulations. Your task is to provide legally accurate and precise responses for a non-legal expert, based **only** on the provided legal text. **Do not make assumptions** or provide information outside the given context.\n",
    "\n",
    "Legal Text:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Initialize ChromaDB client and collection\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb\")  # Ensure this points to your ChromaDB path\n",
    "collection = chroma_client.get_collection(\"df_810_bart\")  # Use your collection name\n",
    "\n",
    "# Load the embedding model (BART)\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartModel.from_pretrained(model_name)\n",
    "\n",
    "# Initialize Anthropic client for Claude\n",
    "claude_client = anthropic.Anthropic(api_key)\n",
    "\n",
    "# Use Metal (MPS) if available\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)  # Move the embedding model to the device\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Generate embeddings using BART.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "def retrieve_docs(query, top_k=3):\n",
    "    \"\"\"Retrieve relevant documents from ChromaDB.\"\"\"\n",
    "    query_embedding = embed_text(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "    return results[\"documents\"][0] if results[\"documents\"] else []\n",
    "\n",
    "def generate_answer(query):\n",
    "    \"\"\"Retrieve documents and use Claude to generate an answer.\"\"\"\n",
    "    retrieved_docs = retrieve_docs(query)\n",
    "    context = \"\\n\".join(retrieved_docs) if retrieved_docs else \"No relevant documents found.\"\n",
    "\n",
    "    # Format the prompt\n",
    "    formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "\n",
    "    # Generate the response using Claude\n",
    "    response = claude_client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",  # Use the correct model name\n",
    "        max_tokens=1024,\n",
    "        system=\"You are an AI assistant specializing in NYC legal regulations.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's response from the returned messages\n",
    "    assistant_message = response.content[0].text\n",
    "\n",
    "    return assistant_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae0eea-be23-4acb-89ee-03b0f69562d3",
   "metadata": {},
   "source": [
    "# RAG #2: QUERY THE ANTHROPIC GENERATED ANSWER HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fecf3-01c4-41af-bca9-09f478dbc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query\n",
    "query = \"What are the restrictions on carrying guns in NYC?\"\n",
    "answer = generate_answer(query)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
